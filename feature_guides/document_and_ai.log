I have this project with details below:

I also have two AI codebases, one for cv extraction into structured json endpoint from a document, pdf/docx. and one for analysing extracted json against a job spec.

I have no idea how and where to plug them in into the codebase/project for candidate side & employer side, also in a way that is the top 500 fortune company level solution best usecase and save costs because requests to third party ai apis costs money.

Here are my preliminaries ideas:

##Document Extraction service (Candidate side)

: The project current uses a different json spec from the one use in in the document extraction service. Plan is to refactor document service a little bit to accept the schema of the project.

Document service use cases on the candidate side: Extract CV into structured JSON to fill up the candidate profile (as the resume structure json on the project is long). We can need to limit use of this feature per candidate in a way that makes economic sense and good experience and innovative experience for the candidate. This will help them and making it encouraging to fill up the long profile on the platform.

##Document Service (Employer side): it can be adapted to automatically extract jobs from employer uploaded documents either pdf/docx. to also make the employer side experience better.

The document service can also be made available as a feature to employers to analyse resumes on employer dashboard (economic ratess limits would need to be considered).

##TalentAnalyser service

This one is very costly when it comes to requests to the API of third party AI, but use cases are pretty insane, it can analyse entire candidate profile against a job description and provide full match analysis accross skills, education, experience, overall analysis and even recommendations.

Use case here, I imagine employers can set it as a requirement to the job description that candidates cannot applied until they did an ai analysis, Even though employers can indicate that they will still use humans to review the application.

So from employer once candidate has applied I imagine being presented with 3 views for their profile: Candidate snapshot profile (at the time of application), candidate live profile (if candidate indicates it can be discoverable), and an AI analysis screen for employer to view full analysis against the job and candidate, including all recommendations from AI. This can make it much faster for employers to get to hiring the candidate.

(Candidate Side), analyser service can analyse the job against their profile and later that match/score (in terms of stars can be displayed on the logged in candidate next to the job cards. I don't know how many times that makes economic sense a candidate can manually trigger the analysis or if it should be automated. Also analysis takes around 1-2 minutes.

Can I show you the Project details and also the codebases of the services, starting with document service extract to see how we can adapt them for this use case? I can also share a open api documentation of the project. Analyse


Absolutely! I'd love to help you architect this solution.
Yes, please share the following in order:

Project details - architecture, tech stack, current data models/schemas
OpenAPI documentation - this will help me understand your existing endpoints and data structures
Document Extraction Service codebase - the CV extraction service
TalentAnalyser Service codebase - the matching/analysis service
Initial Thoughts on Your Strategy
Before you share the code, here are some preliminary observations:

âœ… Strong Ideas:
Schema refactoring approach for document service
Rate limiting awareness for cost control
Separate views for employers (snapshot vs live profile vs AI analysis)
Pre-application AI analysis requirement option
ðŸŽ¯ Key Considerations for Fortune 500-Level Implementation:
Cost Optimization Strategies:

Caching analysis results (with TTL based on profile changes)
Batch processing during off-peak hours
Tiered pricing model (basic vs premium features)
Async processing with job queues
Rate Limiting Models:

Candidates: X free extractions/month, then paid or wait period
Employers: Based on subscription tier + pay-per-use overage
Smart throttling (prevent duplicate analyses of same profile+job combo)
When to Trigger Analysis:

Automated: On job application (cached if recent)
Manual: Candidate-initiated preview before applying
Batch: Employer bulk analysis of applicant pool
Please go ahead and share the materials, and I'll provide a detailed integration architecture with cost optimization strategies! ðŸš€

Claude Sonnet 4.5
I will first share the project details and backend api documentation. Note that obviously I won't be strictly following the project details, some I will do more, some less. Do not do anything other than maybe analysing because I will still give you the codebase files:

##Project details
Requirements Document: Etalente-clone
Recruitment Platform
Graduate Project Specification

Project Overview
Etalente-clone is an online recruitment platform designed to streamline the hiring process for organisations. The platform enables hiring managers, recruiters, and candidates to interact efficiently throughout the recruitment workflow, from job posting to hiring decision.

User Roles and Permissions
Hiring Manager:

Create new job posts

View status and details of current job posts

Recruiter:

Advance job posts through recruitment stages (cannot skip steps, but can close at any point)

View and manage applications for job posts

Candidate:

Complete a personal profile, including selecting and adding skills

Apply for job posts (requires authentication)

Job Post Lifecycle
Each job post progresses through the following stages:

Created

Published

Applied

Phone Screening

Interview

Offered

Hired / Rejected / Closed (final state)

Rules:

Multiple candidates can apply for any published job post.

Only recruiters can advance posts between stages (no skipping steps).

Job posts can be closed by recruiters at any stage.

Candidate Profile & Application Workflow
Candidates must complete their profile before applying for any job.

Profile includes: Name, Contact Details and a small bio

Personal details

Skill list (type and select from existing options, or add new skills)

Resume upload or input

Application process:

Browse job posts (no login required)

Login (passwordless authentication required) to apply

Track application status through platform notifications or dashboard

Authentication & Security
User authentication must be passwordless (e.g., magic link via email, OAuth, etc.)

Enforce secure access to job application and profile features

No secrets stored in the GitLab repository â€” use gitleaks to scan and ensure compliance

All API calls need to be authenticated and authorized

Technology Stack
Frontend: Angular 20

Backend: Spring Boot (Java 21), built with Gradle

Persistence Storage: Developer's choice (e.g., PostgreSQL, MySQL, DynamoDB, MongoDBâ€¦) â€” must integrate with Spring Boot

All schema changes must be done using SQL migrations

Containerization: Docker for both frontend and backend

Source Control: GitLab, using GitLab pipelines

Deployment: AWS (using ECS, EKS, or Elastic Beanstalk, as suitable)

Testing Requirements
Frontend:

Unit tests

Acceptance tests

Backend:

Unit tests

Acceptance tests

OpenAPI documentation auto-generated for all endpoints

Continuous Integration & Deployment (CI/CD)
GitLab repository with structured code and documentation

Scan repository with gitleaks for secrets before building

Build frontend and backend applications

Run automated tests (unit and acceptance)

View test results

Package applications into Docker containers

Deploy containers to AWS environment

Performance tests (Optional)

Additional Recommendations & Useful Details
Include clear error messages and guidance for all user actions

Ensure a responsive and accessible UI following best practices

Audit logs for key actions (job post creation, status changes, applications)

Implement notification system (email or in-app) for application status updates (Optional)

Design a visually appealing and intuitive interface

Document API usage and provide sample requests/responses in OpenAPI docs

Automated backup routines for persistence storage

Suggested images:

Sample dashboard mockups for candidates, recruiters, and hiring managers

Notification pop-up and audit log sample screens

Backup schedule flowchart

Non-functional Requirements
System should support at least 100 concurrent users

Response times for user actions should be less than 2 seconds

System should be scalable and easy to maintain

Deliverables
Angular frontend application

Spring Boot backend application with Gradle build scripts

Unit and acceptance test suites for both frontend and backend

OpenAPI documentation for backend API

GitLab repository with CI/CD pipelines

Docker container images for both applications

Deployed system on AWS

Project documentation including architecture diagrams, user guides, and API docs

Documentation
Provide a README that will be useful for any developer who needs to maintain the application

Include architecture diagrams

Planning
Create a working board with all the tasks you will be doing.
We will review in-progress and completed tasks in the review session.

##OpenAPI documentation (this is what I worked on so far, there is still more todo on backend side)

"openapi":"3.1.0","info":{"title":"OpenAPI definition","version":"v0"},"servers":[{"url":"http://localhost:8080","description":"Generated server url"}],"tags":[{"name":"Applicants","description":"Applicant management endpoints for employers"}],"paths":{"/api/profile/update":{"put":{"tags":["profile-controller"],"operationId":"updateProfile","requestBody":{"content":{"application/json":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}},"required":true},"responses":{"200":{"description":"OK"}}}},"/api/profile/me":{"get":{"tags":["profile-controller"],"operationId":"getCurrentUserProfile","responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/JsonNode"}}}}}},"put":{"tags":["profile-controller"],"operationId":"updateFullProfile","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/JsonNode"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/JsonNode"}}}}}}},"/api/profile/me/avatar":{"put":{"tags":["profile-controller"],"operationId":"updateAvatar","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/UpdateAvatarRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/UserDto"}}}}}},"delete":{"tags":["profile-controller"],"operationId":"deleteAvatar","responses":{"200":{"description":"OK"}}}},"/api/profile/complete":{"put":{"tags":["profile-controller"],"operationId":"completeProfile","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/CompleteProfileRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/VerifyTokenResponse"}}}}}}},"/api/organization/logo":{"put":{"tags":["organization-controller"],"operationId":"updateLogo","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/UpdateCompanyLogoRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/OrganizationDto"}}}}}},"delete":{"tags":["organization-controller"],"operationId":"deleteLogo","responses":{"200":{"description":"OK"}}}},"/api/job-posts/{id}":{"get":{"tags":["job-post-controller"],"operationId":"getJobPost","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}}},"put":{"tags":["job-post-controller"],"operationId":"updateJobPost","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/JobPostRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}}},"delete":{"tags":["job-post-controller"],"operationId":"deleteJobPost","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"200":{"description":"OK"}}}},"/api/v1/workflows/{workflowId}/trigger":{"post":{"tags":["workflow-controller"],"operationId":"triggerWorkflow","parameters":[{"name":"workflowId","in":"path","required":true,"schema":{"type":"string"}}],"requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/WorkflowTriggerRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/WorkflowTriggerResponse"}}}}}}},"/api/v1/notifications":{"post":{"tags":["notification-controller"],"operationId":"createNotification","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/NotificationRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/NotificationResponse"}}}}}}},"/api/register":{"post":{"tags":["registration-controller"],"operationId":"initiateRegistration","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/RegistrationRequest"}}},"required":true},"responses":{"200":{"description":"OK"}}}},"/api/register/hiring-manager":{"post":{"tags":["registration-controller"],"operationId":"completeHiringManagerRegistration","parameters":[{"name":"token","in":"query","required":true,"schema":{"type":"string"}}],"requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/HiringManagerRegistrationRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/VerifyTokenResponse"}}}}}}},"/api/register/candidate":{"post":{"tags":["registration-controller"],"operationId":"completeCandidateRegistration","parameters":[{"name":"token","in":"query","required":true,"schema":{"type":"string"}}],"requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/CandidateRegistrationRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/VerifyTokenResponse"}}}}}}},"/api/profile/me/avatar/upload-url":{"post":{"tags":["profile-controller"],"operationId":"getAvatarUploadUrl","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/UploadUrlRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/UploadUrlResponse"}}}}}}},"/api/organization/logo/upload-url":{"post":{"tags":["organization-controller"],"operationId":"getLogoUploadUrl","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/UploadUrlRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/UploadUrlResponse"}}}}}}},"/api/job-posts":{"get":{"tags":["job-post-controller"],"operationId":"listJobPosts","parameters":[{"name":"pageable","in":"query","required":true,"schema":{"ref":"#/components/schemas/Pageable"}},{"name":"search","in":"query","required":false,"schema":{"type":"string"}},{"name":"skillSearch","in":"query","required":false,"schema":{"type":"string"}},{"name":"experienceLevels","in":"query","required":false,"schema":{"type":"array","items":{"type":"string"}}},{"name":"jobTypes","in":"query","required":false,"schema":{"type":"array","items":{"type":"string"}}},{"name":"workTypes","in":"query","required":false,"schema":{"type":"array","items":{"type":"string"}}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/PageJobPostResponse"}}}}}},"post":{"tags":["job-post-controller"],"operationId":"createJobPost","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/JobPostRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}}}},"/api/job-posts/{id}/apply":{"post":{"tags":["job-application-controller"],"operationId":"applyForJob","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/ApplicationSummaryDto"}}}}}}},"/api/invitations/recruiter":{"post":{"tags":["invitation-controller"],"operationId":"inviteRecruiter","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/RecruiterInvitationRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/RecruiterInvitationDto"}}}}}}},"/api/invitations/bulk-recruiter":{"post":{"tags":["invitation-controller"],"operationId":"inviteRecruiterBulk","requestBody":{"content":{"application/json":{"schema":{"type":"array","items":{"ref":"#/components/schemas/RecruiterInvitationRequest"}}}},"required":true},"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}}}}},"/api/invitations/accept/{token}":{"post":{"tags":["invitation-controller"],"operationId":"acceptInvitation","parameters":[{"name":"token","in":"path","required":true,"schema":{"type":"string"}}],"requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/AcceptInvitationRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}}}}},"/api/auth/refresh":{"post":{"tags":["authentication-controller"],"operationId":"refreshToken","parameters":[{"name":"Authorization","in":"header","required":true,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"type":"object","additionalProperties":{}}}}}}}},"/api/auth/logout":{"post":{"tags":["authentication-controller"],"operationId":"logout","responses":{"200":{"description":"OK","content":{"/":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}}}}},"/api/auth/login":{"post":{"tags":["authentication-controller"],"operationId":"login","requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/LoginRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"type":"object","additionalProperties":{"type":"string"}}}}}}}},"/api/applications/{applicationId}/transition":{"post":{"tags":["job-application-controller"],"operationId":"transitionApplicationStatus","parameters":[{"name":"applicationId","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/ApplicationTransitionRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/ApplicationDetailsDto"}}}}}}},"/api/v1/notifications/{notificationId}/read":{"patch":{"tags":["notification-controller"],"operationId":"markAsRead","parameters":[{"name":"notificationId","in":"path","required":true,"schema":{"type":"integer","format":"int64"}}],"responses":{"200":{"description":"OK"}}}},"/api/v1/notifications/user/{userId}/read-all":{"patch":{"tags":["notification-controller"],"operationId":"markAllAsRead","parameters":[{"name":"userId","in":"path","required":true,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK"}}}},"/api/job-posts/{id}/transition":{"patch":{"tags":["job-post-controller"],"operationId":"transitionState","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"requestBody":{"content":{"application/json":{"schema":{"ref":"#/components/schemas/StateTransitionRequest"}}},"required":true},"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}}}},"/api/job-posts/{id}/status":{"patch":{"tags":["job-post-controller"],"operationId":"updateJobPostStatus","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}},{"name":"status","in":"query","required":true,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}},"deprecated":true}},"/api/job-posts/{id}/reopen":{"patch":{"tags":["job-post-controller"],"operationId":"reopenJobPost","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}},{"name":"reason","in":"query","required":false,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}}}},"/api/job-posts/{id}/publish":{"patch":{"tags":["job-post-controller"],"operationId":"publishJobPost","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}}}},"/api/job-posts/{id}/close":{"patch":{"tags":["job-post-controller"],"operationId":"closeJobPost","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}},{"name":"reason","in":"query","required":false,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}}}},"/api/job-posts/{id}/archive":{"patch":{"tags":["job-post-controller"],"operationId":"archiveJobPost","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}},{"name":"reason","in":"query","required":false,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/JobPostResponse"}}}}}}},"/api/invitations/{invitationId}/revoke":{"patch":{"tags":["invitation-controller"],"operationId":"revokeInvitation","parameters":[{"name":"invitationId","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"200":{"description":"OK"}}}},"/api/v1/notifications/user/{userId}":{"get":{"tags":["notification-controller"],"operationId":"getUserNotifications","parameters":[{"name":"userId","in":"path","required":true,"schema":{"type":"string"}},{"name":"pageable","in":"query","required":true,"schema":{"ref":"#/components/schemas/Pageable"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/PageNotificationResponse"}}}}}}},"/api/v1/notifications/user/{userId}/unread-count":{"get":{"tags":["notification-controller"],"operationId":"getUnreadCount","parameters":[{"name":"userId","in":"path","required":true,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"type":"integer","format":"int64"}}}}}}},"/api/register/validate-token":{"get":{"tags":["registration-controller"],"operationId":"validateRegistrationToken","parameters":[{"name":"token","in":"query","required":true,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"type":"object","additionalProperties":{}}}}}}}},"/api/organization/me":{"get":{"tags":["organization-controller"],"operationId":"getCurrentOrganization","responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/OrganizationDto"}}}}}}},"/api/job-posts/{jobId}/applications":{"get":{"tags":["job-application-controller"],"operationId":"getApplicationsForJob","parameters":[{"name":"jobId","in":"path","required":true,"schema":{"type":"string","format":"uuid"}},{"name":"pageable","in":"query","required":true,"schema":{"ref":"#/components/schemas/Pageable"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/PageEmployerApplicationSummaryDto"}}}}}}},"/api/job-posts/{id}/history":{"get":{"tags":["job-post-controller"],"operationId":"getStateHistory","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"type":"array","items":{"ref":"#/components/schemas/StateAuditResponse"}}}}}}}},"/api/job-posts/{id}/available-transitions":{"get":{"tags":["job-post-controller"],"operationId":"getAvailableTransitions","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"type":"array","items":{"type":"string"}}}}}}}},"/api/job-posts/my-posts":{"get":{"tags":["job-post-controller"],"operationId":"listMyJobPosts","parameters":[{"name":"pageable","in":"query","required":true,"schema":{"ref":"#/components/schemas/Pageable"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/PageJobPostResponse"}}}}}}},"/api/invitations":{"get":{"tags":["invitation-controller"],"operationId":"listOrganizationInvitations","parameters":[{"name":"pageable","in":"query","required":true,"schema":{"ref":"#/components/schemas/Pageable"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/PageRecruiterInvitationDto"}}}}}}},"/api/invitations/validate/{token}":{"get":{"tags":["invitation-controller"],"operationId":"validateInvitation","parameters":[{"name":"token","in":"path","required":true,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"type":"object","additionalProperties":{}}}}}}}},"/api/auth/verify":{"get":{"tags":["authentication-controller"],"operationId":"verifyToken","parameters":[{"name":"token","in":"query","required":true,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/VerifyTokenResponse"}}}}}}},"/api/auth/session":{"get":{"tags":["authentication-controller"],"operationId":"checkSession","parameters":[{"name":"Authorization","in":"header","required":false,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"*/*":{"schema":{"ref":"#/components/schemas/SessionResponse"}}}}}}},"/api/applications/{id}":{"get":{"tags":["job-application-controller"],"operationId":"getApplicationDetails","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/ApplicationDetailsDto"}}}}}},"delete":{"tags":["job-application-controller"],"operationId":"withdrawApplication","parameters":[{"name":"id","in":"path","required":true,"schema":{"type":"string","format":"uuid"}}],"responses":{"204":{"description":"No Content"}}}},"/api/applications/me":{"get":{"tags":["job-application-controller"],"operationId":"getMyApplications","parameters":[{"name":"pageable","in":"query","required":true,"schema":{"ref":"#/components/schemas/Pageable"}},{"name":"search","in":"query","required":false,"schema":{"type":"string"}},{"name":"sort","in":"query","required":false,"schema":{"type":"string"}}],"responses":{"200":{"description":"OK","content":{"/":{"schema":{"ref":"#/components/schemas/PageApplicationSummaryDto"}}}}}}},"/api/applicants":{"get":{"tags":["Applicants"],"summary":"Get paginated list of applicants","description":"Retrieve a filtered and paginated list of job applicants for the current organization","operationId":"getApplicants","parameters":[{"name":"pageable","in":"query","description":"Pagination parameters (page, size, sort)","required":true,"schema":{"ref":"#/components/schemas/Pageable"}},{"name":"search","in":"query","description":"General search term (candidate name or job title)","required":false,"schema":{"type":"string","maxLength":100,"minLength":0}},{"name":"skillSearch","in":"query","description":"Search term for candidate skills","required":false,"schema":{"type":"string","maxLength":100,"minLength":0}},{"name":"jobId","in":"query","description":"Filter by specific job post ID","required":false,"schema":{"type":"string","pattern":"^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}"}},{"name":"statuses","in":"query","description":"Filter by application statuses","required":false,"schema":{"type":"array","items":{"type":"string"}}},{"name":"experienceMin","in":"query","description":"Minimum years of experience","required":false,"schema":{"type":"integer","format":"int32","maximum":50,"minimum":0}},{"name":"education","in":"query","description":"Filter by education levels (OR logic)","required":false,"schema":{"type":"array","items":{"type":"string"}}},{"name":"location","in":"query","description":"Filter by location","required":false,"schema":{"type":"string","maxLength":100,"minLength":0}},{"name":"aiMatchScoreMin","in":"query","description":"Minimum AI match score (future feature)","required":false,"schema":{"type":"integer","format":"int32","maximum":100,"minimum":0}}],"responses":{"200":{"description":"Successfully retrieved applicants","content":{"*/*":{"schema":{"ref":"#/components/schemas/PageApplicantSummaryDto"}}}},"400":{"description":"Invalid request parameters","content":{"/":{"schema":{"ref":"#/components/schemas/ErrorResponse"}}}},"403":{"description":"Access denied","content":{"*/*":{"schema":{"ref":"#/components/schemas/ErrorResponse"}}}}}}}},"components":{"schemas":{"JsonNode":{},"UpdateAvatarRequest":{"type":"object","properties":{"profileImageUrl":{"type":"string","minLength":1}},"required":["profileImageUrl"]},"UserDto":{"type":"object","properties":{"id":{"type":"string"},"email":{"type":"string"},"firstName":{"type":"string"},"lastName":{"type":"string"},"role":{"type":"string"},"profileImageUrl":{"type":"string"},"isNewUser":{"type":"boolean"}}},"CompleteProfileRequest":{"type":"object","properties":{"firstName":{"type":"string","maxLength":50,"minLength":0},"lastName":{"type":"string","maxLength":50,"minLength":0},"companyName":{"type":"string","maxLength":100,"minLength":0}},"required":["firstName","lastName"]},"VerifyTokenResponse":{"type":"object","properties":{"token":{"type":"string"},"user":{"ref":"#/components/schemas/UserDto"},"expiresIn":{"type":"integer","format":"int64"}}},"UpdateCompanyLogoRequest":{"type":"object","properties":{"companyLogoUrl":{"type":"string","minLength":1}},"required":["companyLogoUrl"]},"OrganizationDto":{"type":"object","properties":{"id":{"type":"string"},"name":{"type":"string"},"industry":{"type":"string"},"companyLogoUrl":{"type":"string"}}},"JobPostRequest":{"type":"object","properties":{"title":{"type":"string","maxLength":255,"minLength":3},"company":{"type":"string","maxLength":255,"minLength":0},"jobType":{"type":"string","pattern":"Full-time|Part-time|Contract|Internship"},"description":{"type":"string","maxLength":5000,"minLength":50},"location":{"ref":"#/components/schemas/LocationDto"},"remote":{"type":"string","pattern":"Remote|Hybrid|On-site"},"salary":{"type":"string"},"experienceLevel":{"type":"string","pattern":"(?i)Entry-level|Mid-level|Senior-level|Executive"},"responsibilities":{"type":"array","items":{"type":"string"},"maxItems":2147483647,"minItems":1},"qualifications":{"type":"array","items":{"type":"string"},"maxItems":2147483647,"minItems":1},"skills":{"type":"array","items":{"ref":"#/components/schemas/SkillDto"}}},"required":["company","description","qualifications","responsibilities","title"]},"LocationDto":{"type":"object","properties":{"address":{"type":"string"},"postalCode":{"type":"string"},"city":{"type":"string","minLength":1},"countryCode":{"type":"string","minLength":1},"region":{"type":"string"}},"required":["city","countryCode"]},"SkillDto":{"type":"object","properties":{"name":{"type":"string"},"level":{"type":"string"},"keywords":{"type":"array","items":{"type":"string"}}}},"JobPostResponse":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"title":{"type":"string"},"company":{"type":"string"},"jobType":{"type":"string"},"datePosted":{"type":"string"},"description":{"type":"string"},"location":{"ref":"#/components/schemas/JsonNode"},"remote":{"type":"string"},"salary":{"type":"string"},"experienceLevel":{"type":"string"},"responsibilities":{"ref":"#/components/schemas/JsonNode"},"qualifications":{"ref":"#/components/schemas/JsonNode"},"skills":{"ref":"#/components/schemas/JsonNode"},"status":{"type":"string","enum":["DRAFT","OPEN","CLOSED","ARCHIVED"]},"createdByEmail":{"type":"string"},"createdAt":{"type":"string","format":"date-time"},"updatedAt":{"type":"string","format":"date-time"},"applicantsCount":{"type":"integer","format":"int32"},"newApplicantsCount":{"type":"integer","format":"int32"},"companyLogoUrl":{"type":"string"}}},"WorkflowTriggerRequest":{"type":"object","properties":{"subscriberId":{"type":"string","minLength":1},"email":{"type":"string"},"firstName":{"type":"string"},"lastName":{"type":"string"},"payload":{"type":"object","additionalProperties":{}}},"required":["payload","subscriberId"]},"WorkflowTriggerResponse":{"type":"object","properties":{"transactionId":{"type":"string"},"acknowledged":{"type":"boolean"},"status":{"type":"string"}}},"NotificationRequest":{"type":"object","properties":{"type":{"type":"string","minLength":1},"title":{"type":"string","minLength":1},"content":{"type":"string","minLength":1},"recipientId":{"type":"string","minLength":1},"metadata":{"type":"object","additionalProperties":{}}},"required":["content","recipientId","title","type"]},"NotificationResponse":{"type":"object","properties":{"id":{"type":"integer","format":"int64"},"type":{"type":"string"},"title":{"type":"string"},"content":{"type":"string"},"recipientId":{"type":"string"},"status":{"type":"string","enum":["PENDING","SENT","DELIVERED","READ","FAILED"]},"createdAt":{"type":"string","format":"date-time"},"readAt":{"type":"string","format":"date-time"},"metadata":{"type":"object","additionalProperties":{}}}},"RegistrationRequest":{"type":"object","properties":{"email":{"type":"string","format":"email","minLength":1},"role":{"type":"string","enum":["CANDIDATE","RECRUITER","HIRING_MANAGER","ADMIN"]}},"required":["email","role"]},"HiringManagerRegistrationDto":{"type":"object","properties":{"username":{"type":"string","minLength":1},"companyName":{"type":"string","minLength":1},"industry":{"type":"string","minLength":1},"contactPerson":{"type":"string","minLength":1},"contactNumber":{"type":"string","minLength":1,"pattern":"^\\+?[1-9]\\d{1,14}"}},"required":["companyName","contactNumber","contactPerson","industry","username"]},"HiringManagerRegistrationRequest":{"type":"object","properties":{"hiringManager":{"ref":"#/components/schemas/HiringManagerRegistrationDto"}},"required":["hiringManager"]},"CandidateRegistrationDto":{"type":"object","properties":{"username":{"type":"string","minLength":1},"firstName":{"type":"string","minLength":1},"lastName":{"type":"string","minLength":1},"gender":{"type":"string"},"race":{"type":"string"},"disability":{"type":"string"},"contactNumber":{"type":"string","minLength":1,"pattern":"^\\+?[1-9]\\d{1,14}"},"alternateContactNumber":{"type":"string","pattern":"^\+?[1-9]\d{1,14}"}},"required":["contactNumber","firstName","lastName","username"]},"CandidateRegistrationRequest":{"type":"object","properties":{"candidate":{"ref":"#/components/schemas/CandidateRegistrationDto"}},"required":["candidate"]},"UploadUrlRequest":{"type":"object","properties":{"contentType":{"type":"string","minLength":1},"contentLength":{"type":"integer","format":"int64"}},"required":["contentType"]},"UploadUrlResponse":{"type":"object","properties":{"uploadUrl":{"type":"string"},"fileUrl":{"type":"string"},"key":{"type":"string"}}},"ApplicationSummaryDto":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"applicationDate":{"type":"string","format":"date-time"},"status":{"type":"string"},"job":{"ref":"#/components/schemas/JobSummaryDto"}}},"JobSummaryDto":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"title":{"type":"string"},"company":{"type":"string"}}},"RecruiterInvitationRequest":{"type":"object","properties":{"email":{"type":"string","format":"email","minLength":1},"personalMessage":{"type":"string"}},"required":["email"]},"RecruiterInvitationDto":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"email":{"type":"string"},"status":{"type":"string","enum":["PENDING","ACCEPTED","EXPIRED","REVOKED"]},"createdAt":{"type":"string","format":"date-time"},"expiresAt":{"type":"string","format":"date-time"}}},"AcceptInvitationRequest":{"type":"object","properties":{"username":{"type":"string","minLength":1},"firstName":{"type":"string","minLength":1},"lastName":{"type":"string","minLength":1},"contactNumber":{"type":"string","minLength":1,"pattern":"^\\+?[1-9]\\d{1,14}"},"password":{"type":"string"}},"required":["contactNumber","firstName","lastName","username"]},"LoginRequest":{"type":"object","properties":{"email":{"type":"string","format":"email","minLength":1}},"required":["email"]},"ApplicationTransitionRequest":{"type":"object","properties":{"targetStatus":{"type":"string","enum":["APPLIED","UNDER_REVIEW","INTERVIEW_SCHEDULED","OFFER_EXTENDED","HIRED","REJECTED","WITHDRAWN"]}},"required":["targetStatus"]},"ApplicationDetailsDto":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"applicationDate":{"type":"string","format":"date-time"},"status":{"type":"string"},"job":{"ref":"#/components/schemas/JobPostDto"},"candidateProfile":{"ref":"#/components/schemas/CandidateProfileDto"},"communicationHistory":{"type":"array","items":{"ref":"#/components/schemas/CommunicationHistoryDto"}}}},"CandidateProfileDto":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"firstName":{"type":"string"},"lastName":{"type":"string"},"email":{"type":"string"},"profileImageUrl":{"type":"string"},"summary":{"type":"string"},"profile":{"ref":"#/components/schemas/JsonNode"}}},"CommunicationHistoryDto":{"type":"object","properties":{"status":{"type":"string"},"date":{"type":"string","format":"date-time"},"message":{"type":"string"}}},"JobPostDto":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"title":{"type":"string"},"company":{"type":"string"},"jobType":{"type":"string"},"datePosted":{"type":"string"},"description":{"type":"string"},"location":{"ref":"#/components/schemas/JsonNode"},"remote":{"type":"string"},"salary":{"type":"string"},"experienceLevel":{"type":"string"},"responsibilities":{"ref":"#/components/schemas/JsonNode"},"qualifications":{"ref":"#/components/schemas/JsonNode"},"skills":{"ref":"#/components/schemas/JsonNode"},"status":{"type":"string","enum":["DRAFT","OPEN","CLOSED","ARCHIVED"]},"createdByEmail":{"type":"string"},"createdAt":{"type":"string","format":"date-time"},"updatedAt":{"type":"string","format":"date-time"}}},"StateTransitionRequest":{"type":"object","properties":{"targetStatus":{"type":"string","enum":["DRAFT","OPEN","CLOSED","ARCHIVED"]},"reason":{"type":"string"}},"required":["targetStatus"]},"Pageable":{"type":"object","properties":{"page":{"type":"integer","format":"int32","minimum":0},"size":{"type":"integer","format":"int32","minimum":1},"sort":{"type":"array","items":{"type":"string"}}}},"PageNotificationResponse":{"type":"object","properties":{"totalElements":{"type":"integer","format":"int64"},"totalPages":{"type":"integer","format":"int32"},"first":{"type":"boolean"},"last":{"type":"boolean"},"size":{"type":"integer","format":"int32"},"content":{"type":"array","items":{"ref":"#/components/schemas/NotificationResponse"}},"number":{"type":"integer","format":"int32"},"sort":{"ref":"#/components/schemas/SortObject"},"numberOfElements":{"type":"integer","format":"int32"},"pageable":{"ref":"#/components/schemas/PageableObject"},"empty":{"type":"boolean"}}},"PageableObject":{"type":"object","properties":{"offset":{"type":"integer","format":"int64"},"sort":{"ref":"#/components/schemas/SortObject"},"unpaged":{"type":"boolean"},"paged":{"type":"boolean"},"pageNumber":{"type":"integer","format":"int32"},"pageSize":{"type":"integer","format":"int32"}}},"SortObject":{"type":"object","properties":{"empty":{"type":"boolean"},"unsorted":{"type":"boolean"},"sorted":{"type":"boolean"}}},"PageJobPostResponse":{"type":"object","properties":{"totalElements":{"type":"integer","format":"int64"},"totalPages":{"type":"integer","format":"int32"},"first":{"type":"boolean"},"last":{"type":"boolean"},"size":{"type":"integer","format":"int32"},"content":{"type":"array","items":{"ref":"#/components/schemas/JobPostResponse"}},"number":{"type":"integer","format":"int32"},"sort":{"ref":"#/components/schemas/SortObject"},"numberOfElements":{"type":"integer","format":"int32"},"pageable":{"ref":"#/components/schemas/PageableObject"},"empty":{"type":"boolean"}}},"CandidateDto":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"name":{"type":"string"},"email":{"type":"string"},"profileImageUrl":{"type":"string"}}},"EmployerApplicationSummaryDto":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"applicationDate":{"type":"string","format":"date-time"},"status":{"type":"string"},"candidate":{"ref":"#/components/schemas/CandidateDto"}}},"PageEmployerApplicationSummaryDto":{"type":"object","properties":{"totalElements":{"type":"integer","format":"int64"},"totalPages":{"type":"integer","format":"int32"},"first":{"type":"boolean"},"last":{"type":"boolean"},"size":{"type":"integer","format":"int32"},"content":{"type":"array","items":{"ref":"#/components/schemas/EmployerApplicationSummaryDto"}},"number":{"type":"integer","format":"int32"},"sort":{"ref":"#/components/schemas/SortObject"},"numberOfElements":{"type":"integer","format":"int32"},"pageable":{"ref":"#/components/schemas/PageableObject"},"empty":{"type":"boolean"}}},"StateAuditResponse":{"type":"object","properties":{"id":{"type":"string","format":"uuid"},"fromStatus":{"type":"string","enum":["DRAFT","OPEN","CLOSED","ARCHIVED"]},"toStatus":{"type":"string","enum":["DRAFT","OPEN","CLOSED","ARCHIVED"]},"changedByEmail":{"type":"string"},"reason":{"type":"string"},"changedAt":{"type":"string","format":"date-time"}}},"PageRecruiterInvitationDto":{"type":"object","properties":{"totalElements":{"type":"integer","format":"int64"},"totalPages":{"type":"integer","format":"int32"},"first":{"type":"boolean"},"last":{"type":"boolean"},"size":{"type":"integer","format":"int32"},"content":{"type":"array","items":{"ref":"#/components/schemas/RecruiterInvitationDto"}},"number":{"type":"integer","format":"int32"},"sort":{"ref":"#/components/schemas/SortObject"},"numberOfElements":{"type":"integer","format":"int32"},"pageable":{"ref":"#/components/schemas/PageableObject"},"empty":{"type":"boolean"}}},"SessionResponse":{"type":"object","properties":{"authenticated":{"type":"boolean"},"user":{"ref":"#/components/schemas/UserDto"},"expiresIn":{"type":"integer","format":"int64"}}},"PageApplicationSummaryDto":{"type":"object","properties":{"totalElements":{"type":"integer","format":"int64"},"totalPages":{"type":"integer","format":"int32"},"first":{"type":"boolean"},"last":{"type":"boolean"},"size":{"type":"integer","format":"int32"},"content":{"type":"array","items":{"ref":"#/components/schemas/ApplicationSummaryDto"}},"number":{"type":"integer","format":"int32"},"sort":{"ref":"#/components/schemas/SortObject"},"numberOfElements":{"type":"integer","format":"int32"},"pageable":{"ref":"#/components/schemas/PageableObject"},"empty":{"type":"boolean"}}},"ApplicantSummaryDto":{"type":"object","properties":{"id":{"type":"string"},"candidateName":{"type":"string"},"jobTitle":{"type":"string"},"jobId":{"type":"string"},"profileImageUrl":{"type":"string"},"aiMatchScore":{"type":"integer","format":"int32"},"skills":{"type":"array","items":{"type":"string"}},"experienceYears":{"type":"integer","format":"int32"},"location":{"type":"string"},"applicationDate":{"type":"string","format":"date-time"},"status":{"type":"string"}}},"PageApplicantSummaryDto":{"type":"object","properties":{"content":{"type":"array","items":{"ref":"#/components/schemas/ApplicantSummaryDto"}},"pageable":{"ref":"#/components/schemas/PageableObject"},"last":{"type":"boolean"},"totalElements":{"type":"integer","format":"int64"},"totalPages":{"type":"integer","format":"int32"},"first":{"type":"boolean"},"size":{"type":"integer","format":"int32"},"number":{"type":"integer","format":"int32"},"sort":{"$ref":"#/components/schemas/SortObject"},"numberOfElements":{"type":"integer","format":"int32"},"empty":{"type":"boolean"}}},"ErrorResponse":{"type":"object","properties":{"message":{"type":"string"},"errors":{"type":"array","items":{"type":"string"}},"status":{"type":"integer","format":"int32"},"timestamp":{"type":"string","format":"date-time"},"path":{"type":"string"},"details":{"type":"object","additionalProperties":{}}}}}}}

next I will give you the codebase files for document extraction service


Analysis Complete âœ…
Thank you for sharing the project details and OpenAPI documentation. I've analyzed both thoroughly. Here's my high-level assessment before we dive into the codebase integration:

Key Observations
1. Current Architecture
Tech Stack: Spring Boot (Java 21), Angular 20, PostgreSQL/flexible DB, AWS deployment
Authentication: Passwordless (magic links)
User Roles: Candidate, Recruiter, Hiring Manager
Profile Storage: Flexible JSON schema (JsonNode) for candidate profiles
Job Lifecycle: Draft â†’ Open â†’ Closed/Archived (with state transitions)
2. Relevant Endpoints for AI Integration
Candidate Side:
text
PUT /api/profile/me - Update full profile (JsonNode)
PUT /api/profile/complete - Complete profile
GET /api/profile/me - Get current profile
POST /api/job-posts/{id}/apply - Apply for job
GET /api/applications/me - Get my applications
Employer Side:
text
POST /api/job-posts - Create job post
GET /api/job-posts/{jobId}/applications - Get applications
GET /api/applicants - Get filtered applicants list (new endpoint)
3. Data Models to Consider
CandidateProfileDto contains: profile: JsonNode (flexible structure)
JobPostRequest/Response has structured fields (title, description, skills, qualifications, etc.)
ApplicationSummaryDto & ApplicationDetailsDto for application tracking
ApplicantSummaryDto includes aiMatchScore: integer field (already planned!)
Integration Strategy Outline
Document Extraction Service:
Hook into profile completion flow
Transform extracted JSON to match your JsonNode profile schema
Potentially add new endpoint: POST /api/profile/extract or integrate into existing profile endpoints
TalentAnalyser Service:
Create background job system (async processing)
Cache results in database (new table: ai_analysis_results)
Integrate into application flow and employer dashboard
Populate the existing aiMatchScore field in ApplicantSummaryDto
Ready for Codebase Review
I'm ready to analyze your Document Extraction Service codebase. Please share:

Service architecture files (main classes, controllers, DTOs)
JSON schema/output format it currently produces
API endpoints (if it's a separate service)
Any configuration files (especially related to AI provider integration)
This will help me map it precisely to your project's structure and recommend the optimal integration points. ðŸš€

Claude Sonnet 4.5
Here is document extraction files ( note that it uses gemini, if in a use case of batch streaming responses then I don't have the vertex api or can't afford so can also use gemini sdk with api key):

All python models and APIS only:

src/api/endpoints/resume.py
from math import log
from fastapi import APIRouter, Depends, UploadFile
from typing import Dict, Any
import time
from pathlib import Path

from src.core.schemas import ResumeExtractionResponse
from src.core.exceptions import InvalidDocumentTypeError
from src.services.document_classifier import is_resume

from src.services.interfaces import FileStorage, PDFConverter, LLMExtractor
from src.api.dependencies import (
get_file_storage,
get_pdf_converter,
get_llm_extractor,
validate_pdf_file,
get_resume_schema
)
from src.utils.logger import get_logger

router = APIRouter(prefix="/extract", tags=["extract"])

logger = get_logger("ResumeAPI")

@router.post(
"/",
response_model=ResumeExtractionResponse,
summary="[LEGACY] Extract information from resume PDF",
description="Legacy endpoint for PDF resume information extraction. Use /api/v2/extract/resume for template generation."
)
async def extract_resume(
file: UploadFile = Depends(validate_pdf_file),
storage: FileStorage = Depends(get_file_storage),
converter: PDFConverter = Depends(get_pdf_converter),
extractor: LLMExtractor = Depends(get_llm_extractor),
resume_schema: Dict[str, Any] = Depends(get_resume_schema)
) -> ResumeExtractionResponse:
"""
Extract structured information from uploaded PDF resume and generate CV preview.

text
The extraction process:
1. Save uploaded PDF file
2. Convert PDF to markdown
3. Extract information using configured LLM provider
4. Transform data for CV template
5. Generate CV HTML preview
6. Return structured data with CV preview
"""
start_time = time.time()
pdf_path: Path = None

try:
    logger.section(f"Processing Resume: {file.filename}")

    # Save uploaded file
    logger.info("Saving uploaded file")
    pdf_path = await storage.save_upload(file.file, file.filename)

    # Check if the document is a resume before conversion
    # if not is_resume(pdf_path):
    #     raise InvalidDocumentTypeError(
    #         "The uploaded document does not appear to be a resume/CV."
    #     )


    # Convert to markdown
    logger.info("Converting PDF to markdown")
    markdown_content = await converter.convert_to_markdown(pdf_path)
    logger.success(f"PDF converted to markdown ({logger.format_size(len(markdown_content.encode()))})")

    # Extract information using LLM
    logger.info("Extracting structured data using LLM")
    extracted_data = await extractor.extract_resume_data(
        markdown_content,
        resume_schema
    )
    logger.success("Data extraction completed")

    # Generate CV preview and template data
    cv_preview_html = None
    template_data = None

    try:
        logger.info("Generating CV preview")

        # Template generation moved to enhanced endpoint
        logger.info("Legacy endpoint - CV generation not available")
        logger.info("Please use /api/v2/extract/resume for template generation")

    except Exception as cv_error:
        # Legacy endpoint - template generation not supported
        logger.info("Template generation not available in legacy endpoint")
        logger.info("Use /api/v2/extract/resume for full CV generation features")

    processing_time = time.time() - start_time
    logger.success(f"Resume processing completed in {logger.format_time(processing_time)}")

    return ResumeExtractionResponse(
        message="Resume processed successfully",
        extracted_data=extracted_data,
        cv_preview_html=cv_preview_html,
        template_data=template_data,
        processing_time=processing_time
    )

except Exception as e:
    processing_time = time.time() - start_time
    logger.error(f"Resume processing failed after {logger.format_time(processing_time)}")
    logger.error(f"Error: {str(e)}")
    logger.exception("Full error details:")
    raise

finally:
    # Clean up uploaded file
    if pdf_path and pdf_path.exists():
        try:
            await storage.delete(pdf_path)
            logger.info("Temporary file cleaned up")
        except Exception as cleanup_error:
            logger.warning(f"Failed to clean up temporary file: {cleanup_error}")
src/api/dependencies.py
from fastapi import UploadFile, Depends
from typing import Dict, Any

from src.core.config import settings, PDFConverterType
from src.core.exceptions import InvalidFileTypeError
from src.services.interfaces import FileStorage, PDFConverter, LLMExtractor
from src.services.pdf_converter import MarkerPDFConverter
from src.infrastructure.llm.llm_factory import LLMExtractorFactory # Updated import
from src.infrastructure.storage.local import LocalFileStorage

async def get_file_storage() -> FileStorage:
"""Dependency for file storage"""
upload_dir = settings.paths.base_dir / settings.paths.input_pdf_dir
return LocalFileStorage(upload_dir)

async def get_pdf_converter() -> PDFConverter:
"""Dependency for PDF converter - supports both CLI and API"""
if settings.pdf_converter_type == PDFConverterType.API:
# Import here to avoid circular dependency and only when needed
from src.services.pdf_converter_api import MarkerAPIConverter

text
    if not settings.marker_api:
        raise ValueError("Marker API configuration required when pdf_converter_type is 'api'")

    return MarkerAPIConverter(
        api_key=settings.marker_api.api_key,
        base_url=settings.marker_api.base_url,
        timeout=settings.marker_api.timeout,
        max_pages=settings.marker_api.max_pages,
        page_range=settings.marker_api.page_range
    )
else:
    # Default to CLI converter
    temp_dir = settings.paths.base_dir / settings.paths.temp_markdown_dir
    return MarkerPDFConverter(
        marker_executable=settings.paths.marker_executable,
        temp_dir=temp_dir,
        timeout=settings.marker_timeout
    )
async def get_llm_extractor() -> LLMExtractor:
"""Dependency for LLM extractor"""
return LLMExtractorFactory.create_extractor(settings) # Updated to use factory

async def validate_pdf_file(file: UploadFile) -> UploadFile:
"""Dependency to validate uploaded file is PDF"""
if not file.filename.endswith(".pdf"):
raise InvalidFileTypeError("Only PDF files are allowed")
return file

Cache for resume schema - in real app, this might come from DB or config
_resume_schema_cache: Dict[str, Any] = None

async def get_resume_schema() -> Dict[str, Any]:
"""Dependency to get resume extraction schema"""
global _resume_schema_cache
if _resume_schema_cache is None:
# Import here to avoid circular dependency
from src.utils.convert_schema import generate_resume_schema
_resume_schema_cache = generate_resume_schema()
return _resume_schema_cache

src/infrastructure/llm/base_extractor.py
from src.services.interfaces import LLMExtractor
from abc import ABC, abstractmethod
from typing import Dict, Any, Union, Type, Optional
from pydantic import BaseModel, ValidationError
import json
import asyncio
from pathlib import Path
from src.utils.logger import create_logger, Colors
from src.core.exceptions import LLMExtractionError
from src.infrastructure.llm.prompt_manager import PromptManager

Create module logger
logger = create_logger(name)

class BaseLLMExtractor(LLMExtractor, ABC):
"""Base class for LLM extractors with common functionality"""

text
def __init__(
    self,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    temperature: float = 0.0,
    max_new_tokens: int = 2000,
    prompt_name: str = "resume_extraction",
    prompts_dir: Optional[Path] = None
):
    self.max_retries = max_retries
    self.retry_delay = retry_delay
    self.temperature = temperature
    self.max_new_tokens = max_new_tokens
    self.prompt_name = prompt_name

    # Initialize prompt manager
    self.prompt_manager = PromptManager(prompts_dir)

    logger.info(f"[>] Initialized {self.__class__.__name__}")
    logger.debug(f"[>] Max retries: {max_retries}, Temperature: {temperature}")
    logger.debug(f"[>] Using prompt template: {logger.highlight(prompt_name, Colors.CYAN)}")

async def extract_resume_data(
    self,
    markdown_content: str,
    response_model: Union[Type[BaseModel], Dict[str, Any]]
) -> Dict[str, Any]:
    """Common extraction logic with retry mechanism"""
    logger.section("Starting Resume Data Extraction")

    # Handle both Pydantic models and schema dictionaries
    schema, use_pydantic_validation = self._prepare_schema(response_model)

    content_size = len(markdown_content)
    logger.info(f"[>] Content size: {logger.highlight(logger.format_size(content_size), Colors.CYAN)}")
    logger.info(f"[>] Using Pydantic validation: {logger.highlight(str(use_pydantic_validation), Colors.CYAN)}")

    last_exception = None
    for attempt in range(self.max_retries):
        try:
            logger.info(f"\n[>] Attempt {logger.highlight(f'{attempt + 1}/{self.max_retries}', Colors.YELLOW)}")

            # Call provider-specific extraction
            raw_response = await self._do_extraction(markdown_content, schema)

            # Parse JSON response
            logger.info("[>] Parsing JSON response...")
            parsed_data = self._parse_json_response(raw_response)

            # Validate if needed
            if use_pydantic_validation:
                logger.info("[>] Validating response with Pydantic model...")
                validated_data = self._validate_response(parsed_data, self._pydantic_model)
                result = validated_data.model_dump()
            else:
                result = parsed_data

            logger.success("Data extraction successful!")
            logger.info(f"[>] Extracted {logger.highlight(str(len(result)), Colors.GREEN)} fields")
            return result

        except (LLMExtractionError, ValidationError) as e:
            last_exception = e
            if attempt < self.max_retries - 1:
                logger.warning(f"Attempt {attempt + 1} failed: {str(e)[:100]}...")
                logger.info(f"[>] Retrying in {self.retry_delay}s...")
                await asyncio.sleep(self.retry_delay)
            else:
                logger.error(f"All {self.max_retries} attempts failed")
                raise last_exception

def _prepare_schema(
    self,
    response_model: Union[Type[BaseModel], Dict[str, Any]]
) -> tuple[Dict[str, Any], bool]:
    """Prepare schema from Pydantic model or dict"""
    logger.debug("[>] Preparing schema for extraction")

    if isinstance(response_model, type) and issubclass(response_model, BaseModel):
        logger.debug(f"[>] Using Pydantic model: {response_model.__name__}")
        # Store the original model reference for validation later
        self._pydantic_model = response_model
        return response_model.model_json_schema(), True
    elif isinstance(response_model, dict):
        logger.debug("[>] Using dictionary schema")
        self._pydantic_model = None
        return response_model, False
    else:
        logger.error(f"Invalid response_model type: {type(response_model)}")
        raise ValueError(
            f"response_model must be a Pydantic BaseModel class or schema dict, got {type(response_model)}"
        )

@abstractmethod
async def _do_extraction(self, markdown_content: str, schema: Dict[str, Any]) -> str:
    """Provider-specific extraction implementation. Should return raw text response."""
    pass

def _create_prompt(self, markdown_content: str, schema: Dict[str, Any]) -> str:
    """Create extraction prompt using template from file"""
    logger.debug("[>] Creating extraction prompt")

    try:
        # Load prompt template
        template = self.prompt_manager.load_prompt(self.prompt_name)

        # Format with schema and content
        prompt = self.prompt_manager.format_prompt(template, schema, markdown_content)

        logger.debug(f"[>] Prompt length: {logger.format_size(len(prompt))}")
        return prompt

    except Exception as e:
        logger.error(f"Failed to create prompt from template: {e}")
        raise LLMExtractionError(f"Failed to create prompt: {e}")

def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
    """Common JSON parsing logic"""
    response_text = response_text.strip()
    original_length = len(response_text)

    logger.debug(f"[>] Parsing response of length: {original_length}")

    # Remove markdown code blocks if present
    if response_text.startswith("```"):
        parts = response_text.split("```")
        if len(parts) >= 2:
            response_text = parts[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]
            logger.debug("[>] Removed markdown code blocks")

    # Try to find JSON object in response
    json_start = response_text.find('{')
    json_end = response_text.rfind('}') + 1

    if json_start != -1 and json_end > json_start:
        json_str = response_text[json_start:json_end]
        try:
            parsed = json.loads(json_str)
            logger.debug(f"[>] Successfully parsed JSON with {len(parsed)} keys")
            return parsed
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing failed at position {e.pos}: {e.msg}")
            logger.debug(f"[>] Failed JSON excerpt: {json_str[:200]}...")
            raise LLMExtractionError(f"Failed to parse JSON: {e}")
    else:
        logger.error("No JSON object found in response")
        logger.debug(f"[>] Response excerpt: {response_text[:200]}...")
        raise LLMExtractionError("No JSON object found in response")

def _validate_response(self, response: Dict[str, Any], response_model: Type[BaseModel]) -> BaseModel:
    """Validate response using Pydantic model"""
    try:
        validated = response_model(**response)
        logger.debug(f"[>] Validation successful for model: {response_model.__name__}")
        return validated
    except ValidationError as e:
        logger.error(f"Validation failed with {len(e.errors())} errors")
        for error in e.errors()[:3]:  # Show first 3 errors
            logger.error(f"  - {error['loc']}: {error['msg']}")
        raise LLMExtractionError(f"Response validation failed: {e}")

@classmethod
def from_config(cls, config: Any) -> 'BaseLLMExtractor':
    """Create extractor from configuration object"""
    logger.info(f"[>] Creating {cls.__name__} from config")
    # Default implementation - subclasses should override
    return cls(**config.model_dump())
src/infrastructure/llm/prompt_manager.py
from pathlib import Path
from typing import Optional, Dict, Any
import json
from src.utils.logger import create_logger, Colors
from src.core.exceptions import ConfigurationError

logger = create_logger(name)

class PromptManager:
"""Manages loading and formatting of prompts from files"""

text
def __init__(self, prompts_dir: Optional[Path] = None):
    """Initialize prompt manager with prompts directory"""
    if prompts_dir is None:
        # Default to prompts directory relative to this file
        prompts_dir = Path(__file__).parent / "prompts"

    self.prompts_dir = Path(prompts_dir)
    self._cache: Dict[str, str] = {}

    logger.info(f"[>] Initializing PromptManager")  # noqa: F541
    logger.info(f"[>] Prompts directory: {logger.highlight(str(self.prompts_dir), Colors.CYAN)}")

    # Create directory if it doesn't exist
    if not self.prompts_dir.exists():
        logger.warning(f"Prompts directory does not exist: {self.prompts_dir}")
        logger.info("[>] Creating prompts directory...")
        self.prompts_dir.mkdir(parents=True, exist_ok=True)
        logger.success("Prompts directory created")

def load_prompt(self, prompt_name: str = "resume_extraction") -> str:
    """Load a prompt template from file

    Args:
        prompt_name: Name of the prompt file (without .txt extension)

    Returns:
        The prompt template as a string

    Raises:
        ConfigurationError: If prompt file is not found
    """
    # Check cache first
    if prompt_name in self._cache:
        logger.debug(f"[>] Using cached prompt: {prompt_name}")
        return self._cache[prompt_name]

    # Build the prompt path
    prompt_path = self.prompts_dir / f"{prompt_name}.txt"

    logger.debug(f"[>] Looking for prompt at: {prompt_path}")

    if not prompt_path.exists():
        logger.error(f"Prompt file not found: {prompt_path}")
        available_prompts = self.list_available_prompts()
        error_msg = (
            f"Prompt '{prompt_name}' not found at {prompt_path}\n"
            f"Available prompts: {', '.join(available_prompts) if available_prompts else 'None'}"
        )
        raise ConfigurationError(error_msg)

    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt_content = f.read().strip()

        logger.success(f"Loaded prompt '{prompt_name}' ({len(prompt_content)} chars)")

        # Cache the prompt
        self._cache[prompt_name] = prompt_content

        return prompt_content

    except Exception as e:
        logger.error(f"Failed to read prompt file: {e}")
        raise ConfigurationError(f"Failed to load prompt '{prompt_name}': {e}")

def format_prompt(
    self,
    template: str,
    schema: Dict[str, Any],
    content: str,
    **kwargs
) -> str:
    """Format a prompt template with the provided values

    Args:
        template: The prompt template with placeholders
        schema: The JSON schema for extraction
        content: The content to extract from (e.g., resume markdown)
        **kwargs: Additional keyword arguments for template formatting

    Returns:
        The formatted prompt
    """
    try:
        # Convert schema to pretty JSON string
        schema_str = json.dumps(schema, indent=2)

        # Create formatting dictionary
        format_args = {
            'schema': schema_str,
            'content': content,
            **kwargs  # Allow additional custom parameters
        }

        # Format the template
        formatted = template.format(**format_args)

        logger.debug(f"[>] Formatted prompt: {logger.format_size(len(formatted))}")
        return formatted

    except KeyError as e:
        logger.error(f"Missing template variable: {e}")
        raise ConfigurationError(f"Prompt template missing required variable: {e}")
    except Exception as e:
        logger.error(f"Failed to format prompt: {e}")
        raise ConfigurationError(f"Failed to format prompt: {e}")

def list_available_prompts(self) -> list[str]:
    """List all available prompt templates"""
    prompts = []

    try:
        if self.prompts_dir.exists():
            for file in self.prompts_dir.glob("*.txt"):
                prompts.append(file.stem)

        if prompts:
            logger.info(f"[>] Available prompts: {logger.highlight(', '.join(prompts), Colors.CYAN)}")
        else:
            logger.warning("[>] No prompt templates found")

    except Exception as e:
        logger.error(f"Failed to list prompts: {e}")

    return sorted(prompts)

def refresh_cache(self):
    """Clear the prompt cache to force reloading from files"""
    self._cache.clear()
    logger.info("[>] Prompt cache cleared")

def create_prompt_file(self, prompt_name: str, content: str) -> Path:
    """Create a new prompt file (useful for testing)

    Args:
        prompt_name: Name for the prompt file (without .txt extension)
        content: The prompt template content

    Returns:
        Path to the created file
    """
    prompt_path = self.prompts_dir / f"{prompt_name}.txt"

    try:
        with open(prompt_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())

        logger.success(f"Created prompt file: {prompt_path}")

        # Clear cache entry if it exists
        self._cache.pop(prompt_name, None)

        return prompt_path

    except Exception as e:
        logger.error(f"Failed to create prompt file: {e}")
        raise ConfigurationError(f"Failed to create prompt '{prompt_name}': {e}")
Global prompt manager instance
_prompt_manager: Optional[PromptManager] = None

def get_prompt_manager(prompts_dir: Optional[Path] = None) -> PromptManager:
"""Get or create the global prompt manager instance"""
global _prompt_manager
if _prompt_manager is None:
_prompt_manager = PromptManager(prompts_dir)
return _prompt_manager

src/infrastructure/llm/llm_factory.py
from typing import Dict, Type
from src.services.interfaces import LLMExtractor
from src.infrastructure.llm.gemini import GeminiExtractor
from src.infrastructure.llm.sagemaker import SageMakerExtractor
from src.core.config import Settings, LLMProvider
from src.core.exceptions import ConfigurationError

class LLMExtractorFactory:
"""Factory for creating LLM extractors based on configuration"""

text
_extractors: Dict[LLMProvider, Type[LLMExtractor]] = {
    LLMProvider.GEMINI: GeminiExtractor,
    LLMProvider.SAGEMAKER: SageMakerExtractor
}

@staticmethod
def create_extractor(settings: Settings) -> LLMExtractor:
    """Create an LLM extractor based on the configured provider"""
    extractor_class = LLMExtractorFactory._extractors.get(settings.llm_provider)
    if not extractor_class:
        raise ConfigurationError(f"Unsupported LLM provider: {settings.llm_provider}")

    config_attr = settings.llm_provider.value
    config = getattr(settings, config_attr, None)
    if not config:
        raise ConfigurationError(f"{settings.llm_provider.value} configuration not provided")

    return extractor_class.from_config(config)

@staticmethod
def register_extractor(provider: LLMProvider, extractor_class: Type[LLMExtractor]) -> None:
    """Register a new extractor class for a provider"""
    LLMExtractorFactory._extractors[provider] = extractor_class
src/core/schemas.py
from enum import Enum
from pydantic import BaseModel, Field, field_validator
from typing import Dict, Any, Optional, List
from datetime import datetime, timezone, date

class BaseResponse(BaseModel):
"""Base response schema with common fields"""
message: str
timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class ResumeExtractionRequest(BaseModel):
filename: str

text
@field_validator("filename")
@classmethod
def validate_pdf_extension(cls, v):
    if not v.endswith(".pdf"):
        raise ValueError("Only PDF files are allowed")
    return v
class SkillCategory(Enum):
TECHNICAL = "Technical"
SOFT = "Soft"
OTHER = "Other"

class Skill(BaseModel):
name: str
category: SkillCategory = Field(default=None, description="Category of the skill (e.g., Technical, Soft, Other)")

class ResumeExtractionResponse(BaseModel):
message: str
extracted_data: Dict[str, Any]
cv_preview_html: Optional[str] = None
template_data: Optional[Dict[str, Any]] = None
processing_time: Optional[float] = None
timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class ErrorResponse(BaseModel):
error: str
detail: str
timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class Education(BaseModel):
institution: str
degree: Optional[str] = None
field_of_study: Optional[str] = None
start_date: Optional[date] = None
end_date: Optional[date] = None
description: Optional[str] = None
achievements: Optional[List[str]] = []

class Experience(BaseModel):
company: str
position: str
start_date: Optional[date] = None
end_date: Optional[date] = None
description: Optional[str] = None
technologies: List[str] = []
skills: List[Skill] = []

class Project(BaseModel):
name: str
description: Optional[str] = None
technologies: List[str] = []
skills: List[Skill] = []

class Resume(BaseModel):
contact: Optional[Dict] = {}
summary: Optional[str] = None
skills: List[Skill] = []
education: List[Education] = []
experience: List[Experience] = []
projects: List[Project] = []
certifications: List[str] = []

prompt:

You are a resume parser. Extract information from the resume and return a JSON object with the actual data values.

Expected JSON structure:
{schema}

Resume Content:
{content}

IMPORTANT: Return ONLY the extracted data as JSON. Do not include schema keywords like "type", "properties", "title", etc.

Example of CORRECT output:
{{"name": "John Doe", "email": "john@example.com", "skills": ["Python", "SQL"]}}

Example of WRONG output:
{{"type": "object", "properties": {{"name": {{"type": "string", "value": "John Doe"}}}}}}

JSON Output:

gemini prompt:

src/infrastructure/llm/gemini.py
import google.generativeai as genai
from typing import Dict, Any
import asyncio
from src.infrastructure.llm.base_extractor import BaseLLMExtractor
from src.core.exceptions import LLMExtractionError
from src.core.config import GeminiConfig
from src.utils.logger import create_logger, Colors

Create module logger
logger = create_logger(name)

class GeminiExtractor(BaseLLMExtractor):
def init(
self,
api_key: str,
model_name: str,
max_retries: int = 3,
retry_delay: float = 1.0,
temperature: float = 0.0,
max_new_tokens: int = 2000
):
super().init(max_retries, retry_delay, temperature, max_new_tokens)
self.api_key = api_key
self.model_name = model_name

text
    logger.info(f"{logger.highlight('Initializing Gemini Extractor', Colors.CYAN)}")
    logger.info(f"[>] Model: {logger.highlight(model_name, Colors.CYAN)}")

    # Configure Gemini
    genai.configure(api_key=api_key)
    self.model = genai.GenerativeModel(model_name)

    logger.success("Gemini configuration complete")

async def _do_extraction(self, markdown_content: str, schema: Dict[str, Any]) -> str:
    """Gemini-specific extraction implementation"""
    logger.info("[>] Starting Gemini API call...")
    start_time = asyncio.get_event_loop().time()

    try:
        prompt = self._create_prompt(markdown_content, schema)
        prompt_tokens = len(prompt.split())  # Rough estimate
        logger.debug(f"[>] Prompt tokens (estimate): ~{prompt_tokens}")

        # Start chat and send message
        chat = self.model.start_chat()
        logger.debug("[>] Sending message to Gemini...")

        response = await chat.send_message_async(prompt)

        # Calculate response time
        response_time = asyncio.get_event_loop().time() - start_time
        logger.info(f"[>] Response received in {logger.highlight(logger.format_time(response_time), Colors.GREEN)}")

        # Log response details
        response_text = response.text
        response_tokens = len(response_text.split())  # Rough estimate
        logger.debug(f"[>] Response tokens (estimate): ~{response_tokens}")
        logger.debug(f"[>] Response length: {len(response_text)} characters")

        return response_text

    except Exception as e:
        response_time = asyncio.get_event_loop().time() - start_time
        logger.error(f"Gemini API call failed after {logger.format_time(response_time)}")
        logger.error(f"[>] Error type: {type(e).__name__}")
        logger.error(f"[>] Error message: {str(e)}")

        # Handle specific Gemini errors
        if "quota" in str(e).lower():
            logger.error("[!] API quota exceeded")
        elif "invalid" in str(e).lower() and "api" in str(e).lower():
            logger.error("[!] Invalid API key")
        elif "timeout" in str(e).lower():
            logger.error("[!] Request timeout")

        raise LLMExtractionError(f"Gemini API call failed: {e}")

@classmethod
def from_config(cls, config: GeminiConfig) -> 'GeminiExtractor':
    """Create GeminiExtractor from GeminiConfig"""
    logger.info("[>] Creating GeminiExtractor from config")
    logger.debug(f"[>] Config model: {config.model_name}")
    logger.debug(f"[>] Config temperature: {config.temperature}")

    return cls(
        api_key=config.api_key,
        model_name=config.model_name,
        max_retries=config.max_retries,
        retry_delay=config.retry_delay,
        temperature=config.temperature,
        max_new_tokens=config.max_new_tokens
    )
This is the resume schema I want adapted to:

resume.json:

{
"basics": {
"name": "John Doe",
"label": "Programmer",
"image": "",
"email": "john@gmail.com",
"phone": "(912) 555-4321",
"url": "https://johndoe.com",
"summary": "A summary of John Doeâ€¦",
"location": {
"address": "2712 Broadway St",
"postalCode": "CA 94115",
"city": "San Francisco",
"countryCode": "US",
"region": "California"
},
"profiles": [{
"network": "Twitter",
"username": "john",
"url": "https://twitter.com/john"
}]
},
"work": [{
"name": "Company",
"position": "President",
"url": "https://company.com",
"startDate": "2013-01-01",
"endDate": "2014-01-01",
"summary": "Descriptionâ€¦",
"highlights": [
"Started the company"
]
}],
"volunteer": [{
"organization": "Organization",
"position": "Volunteer",
"url": "https://organization.com/",
"startDate": "2012-01-01",
"endDate": "2013-01-01",
"summary": "Descriptionâ€¦",
"highlights": [
"Awarded 'Volunteer of the Month'"
]
}],
"education": [{
"institution": "University",
"url": "https://institution.com/",
"area": "Software Development",
"studyType": "Bachelor",
"startDate": "2011-01-01",
"endDate": "2013-01-01",
"score": "4.0",
"courses": [
"DB1101 - Basic SQL"
]
}],
"awards": [{
"title": "Award",
"date": "2014-11-01",
"awarder": "Company",
"summary": "There is no spoon."
}],
"certificates": [{
"name": "Certificate",
"date": "2021-11-07",
"issuer": "Company",
"url": "https://certificate.com"
}],
"publications": [{
"name": "Publication",
"publisher": "Company",
"releaseDate": "2014-10-01",
"url": "https://publication.com",
"summary": "Descriptionâ€¦"
}],
"skills": [{
"name": "Web Development",
"level": "Master",
"keywords": [
"HTML",
"CSS",
"JavaScript"
]
}],
"languages": [{
"language": "English",
"fluency": "Native speaker"
}],
"interests": [{
"name": "Wildlife",
"keywords": [
"Ferrets",
"Unicorns"
]
}],
"references": [{
"name": "Jane Doe",
"reference": "Referenceâ€¦"
}],
"projects": [{
"name": "Project",
"startDate": "2019-01-01",
"endDate": "2021-01-01",
"description": "Description...",
"highlights": [
"Won award at AIHacks 2016"
],
"url": "https://project.com/"
}]
}

next I will give you the job extraction service


Thought Process
expand
Excellent! Document Service Analyzed âœ…
I've thoroughly analyzed your document extraction service. Before you share the job extraction service, here's what I've noted:

Document Extraction Service - Key Points
Architecture:
Python/FastAPI with async support
Multi-provider LLM (Gemini/SageMaker) via factory pattern
PDF Processing: Marker (CLI or API) â†’ Markdown â†’ LLM extraction
Retry logic: 3 attempts with exponential backoff
Prompt management: File-based templates (prompts/resume_extraction.txt)
Current vs Target Schema Gap:
Current Schema	JSON Resume (Target)	Gap
Simple contact dict	Rich basics + profiles	âš ï¸ Need mapping
Experience	work + volunteer	âš ï¸ Need split logic
Basic skills list	skills with levels/keywords	âš ï¸ Need enrichment
No certifications detail	certificates + awards	âš ï¸ Need extraction
Missing fields	languages, interests, references, publications	âš ï¸ Optional additions
Cost Factors (Gemini):
Token pricing: ~
0.001
p
e
r
1
K
i
n
p
u
t
t
o
k
e
n
s
,

0.001per1Kinputtokens, 0.002 per 1K output tokens
Estimated cost per CV:
0.05
âˆ’
0.05âˆ’0.15 (for 10-30 page resumes)
Retry overhead: 3x cost on failures
Optimization: Cache parsed CVs, batch processing
Integration Strategy Preview
1. Candidate Side - Profile Completion:
text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Angular Upload â”‚â”€â”€PDFâ”€â”€> â”‚ Spring Boot API  â”‚â”€â”€HTTPâ”€â”€>â”‚ Python Extractorâ”‚
â”‚   (Frontend)    â”‚         â”‚ /api/profile/    â”‚         â”‚ FastAPI Service â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   extract        â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                                    â”‚                           â”‚
                                    â”‚                           â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Extract to JSON Resume
                            â”‚  JSON Resume     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚  Mapper Service  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Profile DB     â”‚
                            â”‚ (JsonNode field) â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
2. Employer Side - Job Extraction:
(Waiting for job extraction codebase to design this flow)

Questions Before Job Service:
Does the job extraction service use a similar architecture? (FastAPI + LLM)
What schema does it output? (Need to map to your JobPostRequest)
Does it handle both PDF and DOCX?
Ready for the job extraction service codebase! ðŸ“„âœ¨

Claude Sonnet 4.5
Responses to questions:

Yes
Job schema to map to:
job_schmema.json:

{
"title": "Web Developer",
"company": "Microsoft",
"type": "Full-time",
"date": "2024-07",
"description": "We are looking for a skilled Web Developer to join our team. The role involves building and maintaining web applications.",
"location": {
"address": "1234 GlÃ¼cklichkeit StraÃŸe\nHinterhaus 5. Etage li.",
"postalCode": "10115",
"city": "Berlin",
"countryCode": "DE",
"region": "Berlin"
},
"remote": "Hybrid",
"salary": "100000",
"experience": "Mid-level",
"responsibilities": [
"Develop and maintain web applications",
"Collaborate with cross-functional teams",
"Ensure the technical feasibility of UI/UX designs",
"Optimize applications for maximum speed and scalability"
],
"qualifications": [
"Bachelor's degree in Computer Science or related field",
"3+ years of experience in web development",
"Strong understanding of JavaScript, HTML, and CSS"
],
"skills": [
{
"name": "Web Development",
"level": "Master",
"keywords": [
"HTML",
"CSS",
"JavaScript",
"React",
"Node.js"
]
},
{
"name": "Database Management",
"level": "Intermediate",
"keywords": [
"SQL",
"NoSQL",
"MongoDB"
]
}
],
}

Actually the job description extraction can be a separate endpoint with separate pydandtics and inside document extraction as both use same and prompt. As both document extraction and job post extraction uses same architectecture. So no need to give you the job extraction service.

Thought Process
expand
Perfect! Full Integration Architecture ðŸŽ¯
Now I have the complete picture. Let me provide a comprehensive integration plan.

Phase 1: Document Service Adaptations
1.1 Add New Endpoint for Job Extraction
File: src/api/endpoints/job.py (NEW)

python
from fastapi import APIRouter, Depends, UploadFile
from src.core.schemas import JobExtractionResponse
from src.api.dependencies import (
    get_file_storage,
    get_pdf_converter,
    get_llm_extractor,
    validate_document_file,  # Support PDF + DOCX
    get_job_schema
)

router = APIRouter(prefix="/extract", tags=["extract"])

@router.post(
    "/job",
    response_model=JobExtractionResponse,
    summary="Extract job description from document"
)
async def extract_job(
    file: UploadFile = Depends(validate_document_file),
    storage: FileStorage = Depends(get_file_storage),
    converter: PDFConverter = Depends(get_pdf_converter),
    extractor: LLMExtractor = Depends(get_llm_extractor),
    job_schema: Dict[str, Any] = Depends(get_job_schema)
) -> JobExtractionResponse:
    """Extract structured job information from PDF/DOCX"""
    # Same flow as resume extraction
    # 1. Save file
    # 2. Convert to markdown
    # 3. Extract using LLM
    # 4. Return structured data
1.2 Create Job Schema & Prompt
File: src/core/schemas.py (UPDATE)

python
class JobLocation(BaseModel):
    address: Optional[str] = None
    postalCode: Optional[str] = None
    city: str
    countryCode: str
    region: Optional[str] = None

class JobSkill(BaseModel):
    name: str
    level: Optional[str] = None  # e.g., "Master", "Intermediate"
    keywords: List[str] = []

class JobExtraction(BaseModel):
    title: str
    company: str
    type: str  # Full-time, Part-time, Contract, Internship
    date: Optional[str] = None
    description: str
    location: JobLocation
    remote: str  # Remote, Hybrid, On-site
    salary: Optional[str] = None
    experience: str  # Entry-level, Mid-level, Senior-level
    responsibilities: List[str]
    qualifications: List[str]
    skills: List[JobSkill]

class JobExtractionResponse(BaseModel):
    message: str
    extracted_data: JobExtraction
    processing_time: Optional[float] = None
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
File: src/infrastructure/llm/prompts/job_extraction.txt (NEW)

text
You are a job description parser. Extract information from the job posting and return a JSON object.

Expected JSON structure:
{schema}

Job Posting Content:
{content}

IMPORTANT:
- Return ONLY the extracted data as JSON
- For 'remote' field, use: "Remote", "Hybrid", or "On-site"
- For 'type' field, use: "Full-time", "Part-time", "Contract", or "Internship"
- For 'experience' field, use: "Entry-level", "Mid-level", "Senior-level", or "Executive"
- Extract salary as a string (include currency if mentioned)
- Break down responsibilities and qualifications into separate list items

JSON Output:
1.3 Update Resume Schema to JSON Resume Format
File: src/utils/schema_mapper.py (NEW)

python
"""Maps internal schemas to JSON Resume format"""

def map_to_json_resume(extracted_data: Dict[str, Any]) -> Dict[str, Any]:
    """Transform extracted resume data to JSON Resume format"""

    contact = extracted_data.get("contact", {})

    json_resume = {
        "basics": {
            "name": contact.get("name", ""),
            "label": extracted_data.get("headline", ""),
            "image": "",
            "email": contact.get("email", ""),
            "phone": contact.get("phone", ""),
            "url": contact.get("website", ""),
            "summary": extracted_data.get("summary", ""),
            "location": _extract_location(contact),
            "profiles": _extract_profiles(contact)
        },
        "work": _map_work_experience(extracted_data.get("experience", [])),
        "volunteer": [],  # Can split from experience if indicated
        "education": _map_education(extracted_data.get("education", [])),
        "awards": [],
        "certificates": _map_certificates(extracted_data.get("certifications", [])),
        "publications": [],
        "skills": _map_skills(extracted_data.get("skills", [])),
        "languages": [],
        "interests": [],
        "references": [],
        "projects": _map_projects(extracted_data.get("projects", []))
    }

    return json_resume

def _extract_location(contact: Dict) -> Dict:
    """Extract location from contact info"""
    return {
        "address": contact.get("address", ""),
        "postalCode": contact.get("postal_code", ""),
        "city": contact.get("city", ""),
        "countryCode": contact.get("country_code", ""),
        "region": contact.get("region", "")
    }

def _map_skills(skills: List[Dict]) -> List[Dict]:
    """Map skills to JSON Resume format with levels and keywords"""
    return [
        {
            "name": skill.get("name", ""),
            "level": skill.get("level", ""),
            "keywords": skill.get("keywords", [skill.get("name", "")])
        }
        for skill in skills
    ]
Phase 2: Spring Boot Integration
2.1 Create Document Service Client
File: src/main/java/com/etalente/client/DocumentServiceClient.java (NEW)

java
@Service
@Slf4j
public class DocumentServiceClient {

    private final WebClient webClient;
    private final DocumentServiceConfig config;

    @Autowired
    public DocumentServiceClient(
        WebClient.Builder webClientBuilder,
        DocumentServiceConfig config
    ) {
        this.webClient = webClientBuilder
            .baseUrl(config.getBaseUrl())
            .defaultHeader(HttpHeaders.CONTENT_TYPE, MediaType.APPLICATION_JSON_VALUE)
            .build();
        this.config = config;
    }

    /**
     * Extract resume data from uploaded file
     * @param fileBytes PDF file bytes
     * @param filename Original filename
     * @return Extracted resume data in JSON Resume format
     */
    public Mono<ResumeExtractionResponse> extractResume(
        byte[] fileBytes,
        String filename
    ) {
        MultipartBodyBuilder builder = new MultipartBodyBuilder();
        builder.part("file", new ByteArrayResource(fileBytes))
            .filename(filename)
            .contentType(MediaType.APPLICATION_PDF);

        return webClient.post()
            .uri("/api/v1/extract/resume")
            .body(BodyInserters.fromMultipartData(builder.build()))
            .retrieve()
            .onStatus(HttpStatusCode::is4xxClientError, response ->
                response.bodyToMono(String.class)
                    .flatMap(error -> Mono.error(
                        new DocumentExtractionException("Invalid document: " + error)
                    ))
            )
            .onStatus(HttpStatusCode::is5xxServerError, response ->
                Mono.error(new DocumentExtractionException("Service unavailable"))
            )
            .bodyToMono(ResumeExtractionResponse.class)
            .timeout(Duration.ofMinutes(2))
            .retryWhen(Retry.backoff(3, Duration.ofSeconds(2))
                .filter(throwable -> !(throwable instanceof DocumentExtractionException))
            );
    }

    /**
     * Extract job description from document
     */
    public Mono<JobExtractionResponse> extractJob(
        byte[] fileBytes,
        String filename
    ) {
        // Similar implementation for job extraction
        MultipartBodyBuilder builder = new MultipartBodyBuilder();
        builder.part("file", new ByteArrayResource(fileBytes))
            .filename(filename);

        return webClient.post()
            .uri("/api/v1/extract/job")
            .body(BodyInserters.fromMultipartData(builder.build()))
            .retrieve()
            .bodyToMono(JobExtractionResponse.class)
            .timeout(Duration.ofMinutes(2))
            .retryWhen(Retry.backoff(3, Duration.ofSeconds(2)));
    }
}
2.2 Profile Controller Enhancement
File: src/main/java/com/etalente/controller/ProfileController.java (UPDATE)

java
@RestController
@RequestMapping("/api/profile")
@Slf4j
public class ProfileController {

    private final ProfileService profileService;
    private final DocumentServiceClient documentServiceClient;
    private final RateLimitService rateLimitService;

    /**
     * NEW: Extract resume and populate profile
     * Rate limited per candidate
     */
    @PostMapping("/extract-from-resume")
    @PreAuthorize("hasRole('CANDIDATE')")
    public Mono<ResponseEntity<ProfileExtractionResponse>> extractFromResume(
        @RequestPart("file") FilePart filePart,
        @AuthenticationPrincipal UserDetails userDetails
    ) {
        String userId = userDetails.getUsername();

        // Check rate limit (e.g., 5 extractions per month per candidate)
        return rateLimitService.checkLimit(userId, "RESUME_EXTRACTION", 5, Duration.ofDays(30))
            .flatMap(allowed -> {
                if (!allowed) {
                    return Mono.just(ResponseEntity.status(429)
                        .body(ProfileExtractionResponse.rateLimitExceeded()));
                }

                return filePart.content()
                    .reduce(DataBufferUtils.join())
                    .flatMap(dataBuffer -> {
                        byte[] bytes = new byte[dataBuffer.readableByteCount()];
                        dataBuffer.read(bytes);
                        DataBufferUtils.release(dataBuffer);

                        // Call document extraction service
                        return documentServiceClient.extractResume(bytes, filePart.filename())
                            .flatMap(extractionResponse -> {
                                // Map to profile JsonNode and save
                                JsonNode profileData = mapToProfileSchema(
                                    extractionResponse.getExtractedData()
                                );
                                return profileService.updateProfile(userId, profileData);
                            })
                            .map(profile -> ResponseEntity.ok(
                                ProfileExtractionResponse.success(profile)
                            ));
                    });
            })
            .onErrorResume(DocumentExtractionException.class, e -> {
                log.error("Document extraction failed", e);
                return Mono.just(ResponseEntity.badRequest()
                    .body(ProfileExtractionResponse.error(e.getMessage())));
            });
    }

    private JsonNode mapToProfileSchema(Map<String, Object> extractedData) {
        // Transform JSON Resume format to your profile schema
        ObjectMapper mapper = new ObjectMapper();
        return mapper.valueToTree(extractedData);
    }
}
2.3 Job Post Controller Enhancement
File: src/main/java/com/etalente/controller/JobPostController.java (UPDATE)

java
@RestController
@RequestMapping("/api/job-posts")
public class JobPostController {

    private final DocumentServiceClient documentServiceClient;
    private final JobPostService jobPostService;

    /**
     * NEW: Extract job description from document
     * Available to hiring managers
     */
    @PostMapping("/extract-from-document")
    @PreAuthorize("hasRole('HIRING_MANAGER')")
    public Mono<ResponseEntity<JobExtractionResponse>> extractFromDocument(
        @RequestPart("file") FilePart filePart,
        @AuthenticationPrincipal UserDetails userDetails
    ) {
        return filePart.content()
            .reduce(DataBufferUtils.join())
            .flatMap(dataBuffer -> {
                byte[] bytes = new byte[dataBuffer.readableByteCount()];
                dataBuffer.read(bytes);
                DataBufferUtils.release(dataBuffer);

                return documentServiceClient.extractJob(bytes, filePart.filename())
                    .map(response -> {
                        // Optionally auto-save as draft
                        return ResponseEntity.ok(response);
                    });
            })
            .onErrorResume(e -> {
                log.error("Job extraction failed", e);
                return Mono.just(ResponseEntity.badRequest()
                    .body(JobExtractionResponse.error(e.getMessage())));
            });
    }
}
Phase 3: Cost Optimization Strategy
3.1 Rate Limiting Table
Migration: V1__create_rate_limit_table.sql

sql
CREATE TABLE ai_service_usage (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id VARCHAR(255) NOT NULL,
    service_type VARCHAR(50) NOT NULL, -- 'RESUME_EXTRACTION', 'JOB_EXTRACTION', 'TALENT_ANALYSIS'
    usage_count INT NOT NULL DEFAULT 0,
    period_start TIMESTAMP NOT NULL,
    period_end TIMESTAMP NOT NULL,
    last_used_at TIMESTAMP NOT NULL DEFAULT NOW(),
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),

    CONSTRAINT uk_usage_period UNIQUE (user_id, service_type, period_start)
);

CREATE INDEX idx_usage_lookup ON ai_service_usage(user_id, service_type, period_end);
3.2 Caching Strategy
Table: ai_extraction_cache

sql
CREATE TABLE ai_extraction_cache (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    content_hash VARCHAR(64) NOT NULL UNIQUE, -- SHA-256 of document
    extraction_type VARCHAR(50) NOT NULL, -- 'RESUME', 'JOB'
    extracted_data JSONB NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    accessed_at TIMESTAMP NOT NULL DEFAULT NOW(),
    access_count INT NOT NULL DEFAULT 1
);

CREATE INDEX idx_cache_lookup ON ai_extraction_cache(content_hash, extraction_type);
CREATE INDEX idx_cache_cleanup ON ai_extraction_cache(accessed_at); -- For TTL cleanup
Service: CachedDocumentService.java

java
@Service
public class CachedDocumentService {

    private final DocumentServiceClient documentClient;
    private final ExtractionCacheRepository cacheRepository;

    public Mono<ResumeExtractionResponse> extractResumeWithCache(
        byte[] fileBytes,
        String filename
    ) {
        String contentHash = calculateSHA256(fileBytes);

        // Check cache first
        return cacheRepository.findByContentHashAndType(contentHash, "RESUME")
            .map(cached -> {
                log.info("Cache HIT for resume extraction");
                cached.incrementAccessCount();
                return ResumeExtractionResponse.fromCache(cached.getExtractedData());
            })
            .switchIfEmpty(
                // Cache MISS - call actual service
                documentClient.extractResume(fileBytes, filename)
                    .flatMap(response -> {
                        log.info("Cache MISS - storing extraction result");
                        ExtractionCache cache = new ExtractionCache(
                            contentHash,
                            "RESUME",
                            response.getExtractedData()
                        );
                        return cacheRepository.save(cache)
                            .thenReturn(response);
                    })
            );
    }
}
3.3 Rate Limit Tiers
Configuration: application.yml

yaml
ai-services:
  rate-limits:
    candidate:
      resume-extraction:
        free-tier: 3      # 3 free extractions per month
        premium-tier: 20   # Premium users get 20/month
        period-days: 30
    employer:
      job-extraction:
        per-organization: 50  # 50 job extractions per org per month
        period-days: 30
      resume-analysis:
        per-recruiter: 100    # 100 analyses per recruiter per month
        period-days: 30
Phase 4: Frontend Integration (Angular)
4.1 Candidate Profile - Resume Upload Component
File: src/app/candidate/profile/resume-upload.component.ts

typescript
@Component({
  selector: 'app-resume-upload',
  template: `
    <div class="resume-upload-container">
      <h3>Quick Profile Setup</h3>
      <p>Upload your resume to auto-fill your profile ({{ remainingExtractions }} remaining this month)</p>

      <input type="file"
             accept=".pdf"
             (change)="onFileSelected($event)"
             #fileInput>

      <button (click)="extractAndFill()"
              [disabled]="!selectedFile || isExtracting || remainingExtractions === 0">
        <span *ngIf="!isExtracting">Extract & Fill Profile</span>
        <span *ngIf="isExtracting">
          <mat-spinner diameter="20"></mat-spinner>
          Extracting... (this may take 1-2 minutes)
        </span>
      </button>

      <div *ngIf="extractionResult" class="result-preview">
        <h4>Preview
Claude Sonnet 4.5
continue


Thought Process
expand
typescript
        <h4>Preview Extracted Data</h4>
        <mat-accordion>
          <mat-expansion-panel>
            <mat-expansion-panel-header>
              <mat-panel-title>Basic Info</mat-panel-title>
            </mat-expansion-panel-header>
            <div>{{ extractionResult.basics | json }}</div>
          </mat-expansion-panel>

          <mat-expansion-panel>
            <mat-expansion-panel-header>
              <mat-panel-title>Experience ({{ extractionResult.work?.length }})</mat-panel-title>
            </mat-expansion-panel-header>
            <div *ngFor="let job of extractionResult.work">
              {{ job.position }} at {{ job.name }}
            </div>
          </mat-expansion-panel>

          <mat-expansion-panel>
            <mat-expansion-panel-header>
              <mat-panel-title>Skills ({{ extractionResult.skills?.length }})</mat-panel-title>
            </mat-expansion-panel-header>
            <mat-chip-list>
              <mat-chip *ngFor="let skill of extractionResult.skills">
                {{ skill.name }}
              </mat-chip>
            </mat-chip-list>
          </mat-expansion-panel>
        </mat-accordion>

        <div class="actions">
          <button mat-raised-button color="primary" (click)="saveToProfile()">
            Save to Profile
          </button>
          <button mat-button (click)="editBeforeSaving()">
            Edit Before Saving
          </button>
        </div>
      </div>

      <div *ngIf="error" class="error-message">
        <mat-icon>error</mat-icon>
        {{ error }}
      </div>
    </div>
  `
})
export class ResumeUploadComponent implements OnInit {
  selectedFile: File | null = null;
  isExtracting = false;
  extractionResult: any = null;
  error: string | null = null;
  remainingExtractions = 0;

  constructor(
    private profileService: ProfileService,
    private snackBar: MatSnackBar
  ) {}

  ngOnInit() {
    this.loadRateLimitStatus();
  }

  loadRateLimitStatus() {
    this.profileService.getExtractionQuota().subscribe({
      next: (quota) => {
        this.remainingExtractions = quota.remaining;
      },
      error: (err) => console.error('Failed to load quota', err)
    });
  }

  onFileSelected(event: any) {
    const file = event.target.files[0];
    if (file && file.type === 'application/pdf') {
      this.selectedFile = file;
      this.error = null;
    } else {
      this.error = 'Please select a PDF file';
      this.selectedFile = null;
    }
  }

  extractAndFill() {
    if (!this.selectedFile) return;

    this.isExtracting = true;
    this.error = null;

    this.profileService.extractFromResume(this.selectedFile).subscribe({
      next: (response) => {
        this.isExtracting = false;
        this.extractionResult = response.extractedData;
        this.remainingExtractions--;

        this.snackBar.open(
          'Resume extracted successfully! Review and save.',
          'OK',
          { duration: 5000 }
        );
      },
      error: (err) => {
        this.isExtracting = false;
        if (err.status === 429) {
          this.error = 'Monthly extraction limit reached. Upgrade or wait until next month.';
        } else {
          this.error = err.error?.message || 'Failed to extract resume. Please try again.';
        }
      }
    });
  }

  saveToProfile() {
    this.profileService.updateFullProfile(this.extractionResult).subscribe({
      next: () => {
        this.snackBar.open('Profile updated successfully!', 'OK', { duration: 3000 });
        // Navigate to profile view or reset
      },
      error: (err) => {
        this.error = 'Failed to save profile';
      }
    });
  }

  editBeforeSaving() {
    // Navigate to profile edit page with pre-filled data
    this.profileService.setDraftProfile(this.extractionResult);
    // router.navigate(['/profile/edit']);
  }
}
Service: profile.service.ts

typescript
@Injectable({
  providedIn: 'root'
})
export class ProfileService {
  private apiUrl = '/api/profile';

  constructor(private http: HttpClient) {}

  extractFromResume(file: File): Observable<ResumeExtractionResponse> {
    const formData = new FormData();
    formData.append('file', file);

    return this.http.post<ResumeExtractionResponse>(
      `${this.apiUrl}/extract-from-resume`,
      formData,
      {
        // Show upload progress
        reportProgress: true,
        observe: 'events'
      }
    ).pipe(
      filter(event => event.type === HttpEventType.Response),
      map((event: any) => event.body),
      timeout(120000), // 2 minute timeout
      catchError(this.handleError)
    );
  }

  getExtractionQuota(): Observable<{ used: number; limit: number; remaining: number }> {
    return this.http.get<any>(`${this.apiUrl}/extraction-quota`);
  }

  updateFullProfile(profileData: any): Observable<any> {
    return this.http.put(`${this.apiUrl}/me`, profileData);
  }
}
4.2 Employer - Job Extraction Component
File: src/app/employer/job-posts/job-extraction.component.ts

typescript
@Component({
  selector: 'app-job-extraction',
  template: `
    <mat-card>
      <mat-card-header>
        <mat-card-title>Create Job from Document</mat-card-title>
        <mat-card-subtitle>Upload a job description (PDF or DOCX) to auto-fill</mat-card-subtitle>
      </mat-card-header>

      <mat-card-content>
        <input type="file"
               accept=".pdf,.docx"
               (change)="onFileSelected($event)"
               #fileInput>

        <button mat-raised-button
                color="primary"
                (click)="extractJob()"
                [disabled]="!selectedFile || isExtracting">
          <mat-icon *ngIf="!isExtracting">upload_file</mat-icon>
          <mat-spinner *ngIf="isExtracting" diameter="20"></mat-spinner>
          {{ isExtracting ? 'Extracting...' : 'Extract Job Details' }}
        </button>

        <mat-progress-bar *ngIf="isExtracting"
                          mode="indeterminate">
        </mat-progress-bar>

        <div *ngIf="extractedJob" class="extracted-preview">
          <h3>{{ extractedJob.title }}</h3>
          <p><strong>Company:</strong> {{ extractedJob.company }}</p>
          <p><strong>Location:</strong> {{ extractedJob.location.city }}, {{ extractedJob.location.countryCode }}</p>
          <p><strong>Type:</strong> {{ extractedJob.type }} | {{ extractedJob.remote }}</p>

          <mat-divider></mat-divider>

          <h4>Description</h4>
          <p>{{ extractedJob.description }}</p>

          <h4>Responsibilities ({{ extractedJob.responsibilities?.length }})</h4>
          <ul>
            <li *ngFor="let resp of extractedJob.responsibilities">{{ resp }}</li>
          </ul>

          <h4>Skills Required</h4>
          <mat-chip-list>
            <mat-chip *ngFor="let skill of extractedJob.skills" color="accent">
              {{ skill.name }} ({{ skill.level }})
            </mat-chip>
          </mat-chip-list>

          <div class="actions">
            <button mat-raised-button color="primary" (click)="createJobPost()">
              Create Job Post
            </button>
            <button mat-button (click)="editExtracted()">
              Edit Details
            </button>
          </div>
        </div>
      </mat-card-content>
    </mat-card>
  `
})
export class JobExtractionComponent {
  selectedFile: File | null = null;
  isExtracting = false;
  extractedJob: any = null;

  constructor(
    private jobPostService: JobPostService,
    private router: Router,
    private snackBar: MatSnackBar
  ) {}

  onFileSelected(event: any) {
    this.selectedFile = event.target.files[0];
  }

  extractJob() {
    if (!this.selectedFile) return;

    this.isExtracting = true;

    this.jobPostService.extractFromDocument(this.selectedFile).subscribe({
      next: (response) => {
        this.isExtracting = false;
        this.extractedJob = response.extractedData;
        this.snackBar.open('Job details extracted!', 'OK', { duration: 3000 });
      },
      error: (err) => {
        this.isExtracting = false;
        this.snackBar.open('Extraction failed: ' + err.message, 'Close', { duration: 5000 });
      }
    });
  }

  createJobPost() {
    // Map extracted data to JobPostRequest format
    const jobPostRequest = this.mapToJobPostRequest(this.extractedJob);

    this.jobPostService.createJobPost(jobPostRequest).subscribe({
      next: (createdJob) => {
        this.snackBar.open('Job post created!', 'OK', { duration: 3000 });
        this.router.navigate(['/employer/jobs', createdJob.id]);
      },
      error: (err) => {
        this.snackBar.open('Failed to create job post', 'Close');
      }
    });
  }

  editExtracted() {
    // Navigate to job creation form with pre-filled data
    this.router.navigate(['/employer/jobs/new'], {
      state: { extractedData: this.extractedJob }
    });
  }

  private mapToJobPostRequest(extracted: any): any {
    return {
      title: extracted.title,
      company: extracted.company,
      jobType: extracted.type,
      description: extracted.description,
      location: {
        address: extracted.location.address,
        postalCode: extracted.location.postalCode,
        city: extracted.location.city,
        countryCode: extracted.location.countryCode,
        region: extracted.location.region
      },
      remote: extracted.remote,
      salary: extracted.salary,
      experienceLevel: extracted.experience,
      responsibilities: extracted.responsibilities,
      qualifications: extracted.qualifications,
      skills: extracted.skills.map((s: any) => ({
        name: s.name,
        level: s.level,
        keywords: s.keywords
      }))
    };
  }
}
Phase 5: TalentAnalyser Integration (Next Steps)
Before diving into the TalentAnalyser, I need to see:

TalentAnalyser codebase - to understand:

Input format (candidate profile + job description)
Output format (match analysis structure)
Processing time and cost per analysis
API endpoints
Analysis requirements:

Should we store analysis results in DB?
How long should cached analysis be valid?
What triggers re-analysis?
Phase 6: Cost Management Dashboard
Backend Endpoint: Monitor AI Usage
java
@RestController
@RequestMapping("/api/admin/ai-usage")
@PreAuthorize("hasRole('ADMIN')")
public class AIUsageController {

    @GetMapping("/statistics")
    public Mono<ResponseEntity<UsageStatistics>> getUsageStats(
        @RequestParam(required = false) LocalDate startDate,
        @RequestParam(required = false) LocalDate endDate
    ) {
        return aiUsageService.getStatistics(startDate, endDate)
            .map(ResponseEntity::ok);
    }

    @GetMapping("/costs")
    public Mono<ResponseEntity<CostBreakdown>> getCostBreakdown(
        @RequestParam String period // "daily", "weekly", "monthly"
    ) {
        return costCalculator.calculateCosts(period)
            .map(ResponseEntity::ok);
    }
}
Cost Estimation Model:

java
@Service
public class CostEstimationService {

    // Gemini Flash pricing (example)
    private static final double INPUT_TOKEN_COST = 0.00000075;  // $0.00075 per 1K tokens
    private static final double OUTPUT_TOKEN_COST = 0.000003;   // $0.003 per 1K tokens

    public CostEstimate estimateResumeExtractionCost(int pageCount) {
        // Average: 500 tokens per page input, 2000 tokens output
        double inputTokens = pageCount * 500;
        double outputTokens = 2000;

        double inputCost = (inputTokens / 1000) * INPUT_TOKEN_COST;
        double outputCost = (outputTokens / 1000) * OUTPUT_TOKEN_COST;

        return new CostEstimate(
            inputCost + outputCost,
            pageCount,
            "RESUME_EXTRACTION"
        );
    }

    public MonthlyProjection projectMonthlyCosts(
        int expectedResumes,
        int expectedJobExtractions,
        int expectedAnalyses
    ) {
        double resumeCosts = expectedResumes * estimateResumeExtractionCost(3).getTotalCost();
        double jobCosts = expectedJobExtractions * 0.02; // Avg $0.02 per job
        double analysisCosts = expectedAnalyses * 0.05;  // Avg $0.05 per analysis

        return new MonthlyProjection(
            resumeCosts,
            jobCosts,
            analysisCosts,
            resumeCosts + jobCosts + analysisCosts
        );
    }
}
Implementation Roadmap
Week 1: Foundation
âœ… Adapt document service schemas (JSON Resume format)
âœ… Add job extraction endpoint to Python service
âœ… Create Spring Boot client for document service
âœ… Implement basic rate limiting
Week 2: Integration
ðŸ”¨ Add profile extraction endpoint to backend
ðŸ”¨ Add job extraction endpoint to backend
ðŸ”¨ Implement caching layer
ðŸ”¨ Frontend: Resume upload component
Week 3: TalentAnalyser (Pending codebase)
ðŸ”¨ Integrate talent matching service
ðŸ”¨ Implement analysis caching
ðŸ”¨ Add match score to applicant list
ðŸ”¨ Create analysis dashboard for employers
Week 4: Optimization & Testing
ðŸ”¨ Load testing for AI services
ðŸ”¨ Cost monitoring dashboard
ðŸ”¨ Rate limit fine-tuning
ðŸ”¨ End-to-end testing
Critical Configuration Files
Python Service (config.yaml):
yaml
server:
  host: "0.0.0.0"
  port: 8000
  workers: 4

llm:
  provider: "gemini"  # or "sagemaker"
  gemini:
    api_key: ${GEMINI_API_KEY}
    model: "gemini-1.5-flash"
    temperature: 0.0
    max_tokens: 2000
    max_retries: 3

pdf_converter:
  type: "api"  # or "cli"
  marker_api:
    base_url: ${MARKER_API_URL}
    api_key: ${MARKER_API_KEY}
    timeout: 60

cache:
  enabled: true
  ttl_days: 30
  max_size_mb: 1000

rate_limits:
  resume_extraction_per_user_per_month: 3
  job_extraction_per_org_per_month: 50
Spring Boot (application.yml):
yaml
document-service:
  base-url: ${DOCUMENT_SERVICE_URL:http://localhost:8000}
  timeout:
    connection: 30s
    read: 120s
  retry:
    max-attempts: 3
    backoff: 2s
  cache:
    enabled: true
    ttl: 30d

ai-services:
  cost-limits:
    monthly-budget-usd: 500
    alert-threshold: 0.8
    auto-disable-at: 0.95

  rate-limits:
    candidate-resume-extraction: 3/month
    employer-job-extraction: 50/month
    resume-analysis: 100/month
